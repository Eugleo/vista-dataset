# Directory task definitions (yaml) and task data (json) are stored
task_dir: tasks/habitat
# Directory where the videos are stored
# In task data, the video path is relative to this directory
video_dir: videos/habitat
# The tasks to test
# The evaluation script will look for [task_name].yaml in task_dir
tasks:
  - open_cabinet
# The models to test
models:
  # We support two kinds of models: encoder and gpt
  - kind: encoder
    # Which encoder to use
    # We support: viclip, s3d, clip
    encoder: viclip
    # Which head(s) to use (currently only cosine is supported)
    heads:
      - kind: cosine
    # Optional, is 8 by default
    batch_size: 16
  - kind: encoder
    encoder: s3d
    heads:
      - kind: cosine
  - kind: encoder
    encoder: clip
    heads:
      - kind: cosine
    # CLIP models require the specific model that is to be used
    hf_model: ViT-bigG-14/laion2b_s39b_b160k
    # CLIP models also require the number of frames to average over
    n_frames: 32
  # - kind: gpt
  #   # GPT models require the number of frames that are used as input
  #   n_frames: 5